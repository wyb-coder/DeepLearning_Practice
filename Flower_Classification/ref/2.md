## 花卉分类比赛：落地训练与提交方案 v2（整合 + 细化 + 不均衡/缩放/早停补充）



---

## 1. 赛题与边界条件

- 训练集：`data/train/`；标注文件 `data/train_labels.csv`（至少包含：`filename, category_id`；可附带 `chinese_name, english_name`，训练时忽略）。
- 评测入口：评测方运行 `code/predict.py`，对 `test_dataset/*.jpg` 推理，输出 `results/results/submission.csv`。提交列与顺序以评测方要求为准；本仓库参考样例为 `ref/predictions_sample.csv`（`filename,category_id,confidence`），若仅需两列请导出 `filename,category_id`。
- 允许：预训练权重、数据增强、TTA、融合；不允许外部数据。
- 评测关注：Clean Top-1 与鲁棒性（对常见扰动的稳定性）。

- 补充：
(1)训练过程及最终模型保存到results/model下
(2)假设所需要的预训练模型已经保存至results/model/pretrained_model/下
所需要的预训练模型分别为：
convnextv2_base.fcmae_ft_in22k_in1k
vit_base_patch16_224.augreg2_in21k_ft_in1k

---

## 2. 数据统计、划分与复现

- 类别统计：从 `train_labels.csv` 计算类别数 C、每类样本数 n_c，记录长尾程度（建议输出到 CSV/日志）。
- 划分策略：默认 5 折 Stratified K-Fold（按 `category_id` 分层）。
- 标签映射：构建 `category_id -> class_idx(0..C-1)` 与其逆映射；训练/评估用 `class_idx`；提交时用原始 `category_id` 恢复。
- 近重复样本防泄漏【落实】：可选 pHash 或特征聚类先分组，再按组做分层 K 折（资源有限时可跳过）。
- 复现性【落实】：
	- 统一随机种子（Python/NumPy/PyTorch）；
	- `cudnn.deterministic=True`，`cudnn.benchmark=False`；
	- 固化划分索引、类别映射与配置快照，训练日志与权重带时间戳/折号/种子；
	- 建议将上述元信息保存至 `results/results/exp_name/meta/`。

---

## 3. 预处理与数据增强（含渐进分辨率与鲁棒增强）

- 输入尺寸（渐进式）【落实】：前半程 224；后半程升至 320/384；若显存允许，可 448 精修 5–10 epoch。
- 训练增强（默认以 320 为例）：
	- RandomResizedCrop(320, scale=0.8~1.0)
	- RandomHorizontalFlip(p=0.5)
	- RandomVerticalFlip(p=0.1)、RandomRotation(±10°)
	- ColorJitter(0.2, 0.2, 0.2, 0.1)
	- ToTensor + Normalize(ImageNet 均值方差)
	- Random Erasing(p=0.1)【落实】
	- Mixup(α=0.2) + CutMix(α=0.8)，总强度≈0.4【落实】
	- 可选低强度 AugMix 或 ImageNet-C 风格扰动的少量采样【落实】
- 验证/测试：Resize(短边≈1.14×输入) + CenterCrop(输入尺寸) + Normalize。
- TTA【落实】：多尺度（{320, 384}）× 水平翻转；logit 均值；色彩不作强增强以避免分布偏移。

---

## 4. 模型与分类头

- 主干首选：ConvNeXtV2-Base（`convnextv2_base.fcmae_ft_in22k_in1k`）或 EfficientNetV2-M；
- 第二骨干（融合增益）：ViT-B/16（EVA/CLIP 权重）或 Swin；
- 分类头：默认 Linear；可选 ArcFace/CosFace（margin 0.2~0.4）对比消融；
- 位置编码与归一化需与预训练对齐（ViT 注意插值策略）。

---

## 5. 类别不均衡处理（新增补充：采样策略 + 损失层配套）

以当前 CSV 的真实分布为准（C 为 `unique(category_id)` 数量）。少数类仍可能受影响，建议“数据层 + 损失层”二选一或组合，避免过度矫正。

### 5.1 数据层采样策略

1) WeightedRandomSampler（按类频权重抽样）
	- 简单权重：$w_c = \frac{1}{n_c}$；样本权重为其类别权重；
	- 有效样本数（Class-Balanced by Effective Number）【推荐】：
		$$E(n_c) = \frac{1 - \beta^{n_c}}{1 - \beta},\quad w_c \propto \frac{1}{E(n_c)},\ \beta\in[0.9,0.999]$$
		默认取 $\beta=0.999$，对中等长尾更稳。

2) Class-Balanced Sampler（每个 mini-batch 尽量各类均衡）
	- 以“类”为单位轮转抽样，或每 epoch 保证各类采样次数近似相同；
	- 对样本少的类别执行“过采样”，对样本多的类别允许轻微“欠采样”；
	- 防止过拟合：设置过采样上限（例如不超过原样本数 3×）；并与 Mixup/CutMix 协同使用以缓解重复带来的过拟合。

3) Repeat Factor/阈值过采样（轻量）
	- 给定阈值 t 与指数 $\alpha$，令：$R_c = \max\big(1, (\tfrac{t}{n_c})^{\alpha}\big)$，将少数类重复若干次（取整）；
	- 典型：$t=\operatorname{median}(n_c)$，$\alpha\in[0.3,0.7]$。


### 5.2 损失层策略

- Label Smoothing Cross Entropy（LSCE，默认 $\epsilon=0.05$）
- Focal Loss（$\gamma\in[1,2]$，可用于难类提升，但与强增强同时用需谨慎）
- Balanced Softmax Cross Entropy（BSCE）【类先验修正】：
	$$p(y=c\mid x)=\frac{n_c\,e^{z_c}}{\sum_j n_j\,e^{z_j}},\ \ \mathcal{L}=-\log p(y=c\mid x)$$
	其中 $z_c$ 为类别 c 的 logit，$n_c$ 为该类训练样本数。BSCE 与均衡采样不建议叠加。

评估与告警：同时监控 Macro-Averaged Top-1/F1，以发现对少数类的退化。

---

## 6. 优化器、学习率与批量自适应缩放（新增补充：统一公式）

- 优化器：AdamW（默认）；SGD 可作为对照；
- 权重衰减：0.05（ViT/Swin 可 0.05~0.1）；
- 学习率基准与线性缩放【落实】：
- 热身与调度【落实】：Warmup 5 epoch（线性升至目标 lr）+ Cosine Decay；
- 分层学习率（LLRD）【落实】（示例，ConvNeXt/ViT 通用）：
	- 前层×0.25，中层×0.5，后层×1.0，分类头×1.5；
	- ViT 可采用“逐步解冻”：阶段性放开更多 block 的训练。
- AMP 混合精度 + 梯度裁剪（clip_norm=1.0）。

---



### 7 早停与模型选择（新增补充）

- 监控指标：
	- Clean 验证集 Top-1（`val_acc_clean`）；
	- 可选“腐蚀验证集”Top-1（`val_acc_corr`）；
	- 综合评分（默认作为 best 选择依据）：
		$$S = 0.7\cdot \text{val\_acc\_clean} + 0.3\cdot \text{val\_acc\_corr}$$
- 早停策略：
	- `patience=10`（epoch 级），`min_delta=0.001`；
	- 监控指标：默认用综合评分 S；若未启用腐蚀集则用 `val_acc_clean`；
	- 满足“连续 `patience` 个 epoch 无显著提升（< min_delta）”则提前停止该折训练；
	- Tie-breaker：若分数相等，优先选择更早出现的权重；
	- EMA 优先：每次评估对“原模型与 EMA”都计算指标，保存分数更高的一方为 best。若 EMA 失效（罕见），回退到原模型 best。
- Checkpoint 规范：

### 7.2 渐进分辨率节奏（示例）

- Epoch 1–15：224，Mixup/CutMix 正常；
- Epoch 16–40：320 或 384；
- （可选）Epoch 41–48：448，关闭或减弱 Mixup/CutMix，仅做短程精修；
- 升维时将学习率按 Cosine 连续衔接；若显存紧张，可减小 batch 并依线性缩放公式微调 lr。

---


## 9. 推理、TTA、融合与温度缩放（与 1.md 对齐并细化）

- 预测入口：`code/predict.py`；预处理与训练对齐；
- TTA【落实】：多尺度（320、384）× 水平翻转；logit 求均值；
- 多折/多种子/多骨干融合【落实】：发现 `results/model/*.pth`（或按命名模式），加载逐个前向、logit 平均；
- 温度缩放（Temperature Scaling）【落实】：
	- 在每折验证集拟合温度 T（最小化 NLL），推理时对融合后的 logit 除以 T；
	- 先在 Clean 上拟合 T；若鲁棒集权重较高，可对综合验证集拟合；
- 

---

## 11. 默认关键超参（可直接使用）

- 模型：ConvNeXtV2-Base（主） + ViT-B/16（辅融合）
- 输入尺寸：224（前 15 epoch）→ 320/384（后 25 epoch）→（可选）448（5–10 epoch）
- 优化器：AdamW；`lr_ref=2e-4`；`weight_decay=0.05`；
- 线性缩放：$\text{lr} = 2\times10^{-4} \times B_{\text{global}}/256$；
- LLRD：前×0.25，中×0.5，后×1.0，头×1.5；
- 正则：LSCE(ε=0.05)，Mixup=0.2，CutMix=0.8，RandomErasing=0.1；
- AMP & Clip：AMP=True，`clip_norm=1.0`；
- EMA：0.9999；SWA：False（可按需开启）；
- 采样：`sampler=weighted_en`，`beta=0.999`（或 `class_balanced`）；过采样上限 3×；
- 早停：`patience=10`，`min_delta=0.001`，`metric=combo`，`alpha=0.7`（Clean:Corr=0.7:0.3）。

---

## 13. 风险与应对（补充不均衡/缩放/早停）

- 过拟合与信息泄漏：严格分层 K 折（可选分组）、增强强度适中、检查评估与 TTA 一致性；
- 少数类退化：启用 Effective-Number WeightedSampler 或 Class-Balanced Sampler；监控 Macro 指标；必要时尝试 BSCE/Focal；
- 学习率不匹配：按线性缩放公式统一；Warmup 5 epoch；如大 batch 不稳，降低比例或增大衰减；
- 训练不稳定：Warmup、LLRD、梯度裁剪、EMA；必要时降低增强强度或学习率；
- 资源与时间：两张 4090 并行不同折/骨干；早停（`patience=10`）显著节省算力；优先稳定基线再做增益实验。

---

